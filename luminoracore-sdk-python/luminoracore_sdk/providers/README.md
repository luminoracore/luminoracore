# LuminoraCore SDK - Providers Module

M√≥dulo de proveedores LLM para el SDK.

---

## üìã Componentes

### 1. BaseProvider (`base.py`)

**Prop√≥sito:** Clase base abstracta para todos los proveedores LLM.

**Caracter√≠sticas:**
- ‚úÖ Interfaz com√∫n para todos los providers
- ‚úÖ Retry autom√°tico con backoff
- ‚úÖ Manejo de errores unificado
- ‚úÖ Soporte para streaming
- ‚úÖ Integraci√≥n con Core Personality (v1.2.0)

**M√©todos Abstractos:**
- `get_default_model() -> str` - Modelo por defecto
- `chat()` - Enviar mensaje
- `stream_chat()` - Stream de mensajes

**M√©todos Opcionales:**
- `chat_with_personality()` - Chat con personalidad (usa Core)
- `stream_chat_with_personality()` - Stream con personalidad (usa Core)

---

### 2. ProviderFactory (`factory.py`)

**Prop√≥sito:** Factory para crear instancias de providers.

**Caracter√≠sticas:**
- ‚úÖ Registro de providers
- ‚úÖ Creaci√≥n desde config
- ‚úÖ Creaci√≥n desde dict
- ‚úÖ Creaci√≥n desde env vars
- ‚úÖ M√∫ltiples providers

**Uso:**
```python
from luminoracore_sdk.providers import ProviderFactory
from luminoracore_sdk.types.provider import ProviderConfig

# Crear desde config
config = ProviderConfig(
    name="openai",
    api_key="your-key",
    model="gpt-3.5-turbo"
)
provider = ProviderFactory.create_provider(config)

# Crear desde dict
provider = ProviderFactory.create_provider_from_dict({
    "name": "openai",
    "api_key": "your-key",
    "model": "gpt-3.5-turbo"
})

# Crear desde env vars
provider = ProviderFactory.create_provider_from_env("openai")
```

---

### 3. Providers Implementados

#### OpenAIProvider (`openai.py`)
- ‚úÖ Modelos: gpt-3.5-turbo, gpt-4, gpt-4-turbo
- ‚úÖ Streaming support
- ‚úÖ Function calling support

#### AnthropicProvider (`anthropic.py`)
- ‚úÖ Modelos: claude-3-sonnet, claude-3-opus, claude-3-haiku
- ‚úÖ Streaming support
- ‚úÖ System messages support

#### DeepSeekProvider (`deepseek.py`)
- ‚úÖ Modelos: deepseek-chat, deepseek-coder
- ‚úÖ OpenAI-compatible API

#### GoogleProvider (`google.py`)
- ‚úÖ Modelos: gemini-pro, gemini-pro-vision
- ‚úÖ Streaming support

#### CohereProvider (`cohere.py`)
- ‚úÖ Modelos: command, command-light
- ‚úÖ Streaming support

#### MistralProvider (`mistral.py`)
- ‚úÖ Modelos: mistral-tiny, mistral-small, mistral-medium
- ‚úÖ Streaming support

#### LlamaProvider (`llama.py`)
- ‚úÖ Modelos locales: llama-2, llama-3
- ‚úÖ OpenAI-compatible API

---

## üîß Uso B√°sico

### Crear Provider

```python
from luminoracore_sdk.providers import OpenAIProvider
from luminoracore_sdk.types.provider import ProviderConfig

config = ProviderConfig(
    name="openai",
    api_key="your-api-key",
    model="gpt-3.5-turbo"
)

provider = OpenAIProvider(config)
```

### Enviar Mensaje

```python
from luminoracore_sdk.types.provider import ChatMessage

messages = [
    ChatMessage(role="system", content="You are a helpful assistant."),
    ChatMessage(role="user", content="Hello!")
]

response = await provider.chat(messages)
print(response.content)
```

### Streaming

```python
async for chunk in provider.stream_chat(messages):
    print(chunk.content, end="", flush=True)
```

---

## üÜï v1.2.0 - Core Integration

### Chat con Personalidad

Los providers ahora pueden usar Core Personality para compilar system prompts:

```python
# Esto usa Core PersonalityCompiler internamente
response = await provider.chat_with_personality(
    personality_data={
        "persona": {
            "name": "Dr. Luna",
            "description": "A scientific assistant"
        },
        "core_traits": {...},
        ...
    },
    user_message="Explain quantum computing",
    conversation_history=[]
)
```

**Requisitos:**
- `luminoracore>=1.2.0` (Core package)
- Se lanza error si Core no est√° disponible

**Internamente:**
1. Crea `Personality` desde Core
2. Usa `PersonalityCompiler` para compilar system prompt
3. Aplica personalidad a la conversaci√≥n

---

## üìä Providers Disponibles

| Provider | Clase | Modelos Default | Streaming |
|----------|-------|-----------------|-----------|
| **OpenAI** | `OpenAIProvider` | gpt-3.5-turbo | ‚úÖ |
| **Anthropic** | `AnthropicProvider` | claude-3-sonnet | ‚úÖ |
| **DeepSeek** | `DeepSeekProvider` | deepseek-chat | ‚úÖ |
| **Google** | `GoogleProvider` | gemini-pro | ‚úÖ |
| **Cohere** | `CohereProvider` | command | ‚úÖ |
| **Mistral** | `MistralProvider` | mistral-tiny | ‚úÖ |
| **Llama** | `LlamaProvider` | llama-2 | ‚úÖ |

---

## üîÑ Retry y Error Handling

Todos los providers incluyen retry autom√°tico:

```python
# Configurar retry en ProviderConfig
config = ProviderConfig(
    name="openai",
    api_key="your-key",
    model="gpt-3.5-turbo",
    extra={
        "timeout": 30,
        "max_retries": 3  # Retry autom√°tico
    }
)
```

**Comportamiento:**
- ‚úÖ Retry con exponential backoff
- ‚úÖ Manejo de rate limits
- ‚úÖ Timeout handling
- ‚úÖ Error logging

---

## üéØ Ejemplo Completo

```python
import asyncio
from luminoracore_sdk.providers import ProviderFactory
from luminoracore_sdk.types.provider import ProviderConfig, ChatMessage

async def main():
    # Crear provider
    config = ProviderConfig(
        name="openai",
        api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-3.5-turbo"
    )
    
    provider = ProviderFactory.create_provider(config)
    
    # Enviar mensaje
    messages = [
        ChatMessage(role="user", content="Hello!")
    ]
    
    response = await provider.chat(messages)
    print(f"Response: {response.content}")
    
    # Streaming
    print("Streaming response:")
    async for chunk in provider.stream_chat(messages):
        print(chunk.content, end="", flush=True)

asyncio.run(main())
```

---

## üêõ Troubleshooting

### Error: "API key is required"

**Soluci√≥n:** Aseg√∫rate de proporcionar la API key:
```python
config = ProviderConfig(
    name="openai",
    api_key="your-key-here"  # ‚úÖ Requerido
)
```

### Error: "Unsupported provider type"

**Soluci√≥n:** Verifica que el provider est√© registrado:
```python
available = ProviderFactory.get_available_providers()
print(f"Available: {available}")
```

### Error: "Core components not available"

**Soluci√≥n:** Solo ocurre si usas `chat_with_personality()`. Instala Core:
```bash
pip install -e ../luminoracore/
```

---

## üìö M√°s Informaci√≥n

- **Client Documentation:** `../client.py`
- **Types:** `../types/provider.py`
- **Core Integration:** `../../luminoracore/tools/compiler.py`

---

**√öltima Actualizaci√≥n:** 2025-11-21  
**Versi√≥n SDK:** 1.2.0  
**Estado:** ‚úÖ M√≥dulo completo y funcionando

