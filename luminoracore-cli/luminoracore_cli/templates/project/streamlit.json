{
  "name": "Streamlit LuminoraCore Project",
  "description": "A Streamlit web application template with LuminoraCore integration",
  "version": "1.0.0",
  "author": "LuminoraCore Team",
  "template_type": "project",
  "files": [
    {
      "path": "app.py",
      "content": "import streamlit as st\nimport json\nfrom luminoracore import LuminoraCore\n\n# Page config\nst.set_page_config(\n    page_title=\"{{project_name}}\",\n    page_icon=\"ðŸ¤–\",\n    layout=\"wide\"\n)\n\n# Initialize LuminoraCore\n@st.cache_resource\ndef get_core():\n    return LuminoraCore()\n\ncore = get_core()\n\n# Main title\nst.title(\"ðŸ¤– {{project_name}}\")\nst.markdown(\"{{description}}\")\n\n# Sidebar\nwith st.sidebar:\n    st.header(\"âš™ï¸ Configuration\")\n    \n    # Provider selection\n    provider = st.selectbox(\n        \"LLM Provider\",\n        [\"openai\", \"anthropic\", \"google\", \"cohere\"],\n        index=0\n    )\n    \n    # Model selection\n    if provider == \"openai\":\n        model = st.selectbox(\n            \"Model\",\n            [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo\"],\n            index=0\n        )\n    elif provider == \"anthropic\":\n        model = st.selectbox(\n            \"Model\",\n            [\"claude-3-sonnet\", \"claude-3-opus\", \"claude-3-haiku\"],\n            index=0\n        )\n    elif provider == \"google\":\n        model = st.selectbox(\n            \"Model\",\n            [\"gemini-pro\", \"gemini-pro-vision\"],\n            index=0\n        )\n    elif provider == \"cohere\":\n        model = st.selectbox(\n            \"Model\",\n            [\"command\", \"command-light\"],\n            index=0\n        )\n    \n    # Personality selection\n    try:\n        personalities = core.list_personalities()\n        personality_options = [p.get(\"name\", \"Unknown\") for p in personalities]\n        \n        if personality_options:\n            selected_personality = st.selectbox(\n                \"Personality\",\n                personality_options,\n                index=0\n            )\n            \n            # Get personality data\n            personality_data = None\n            for p in personalities:\n                if p.get(\"name\") == selected_personality:\n                    personality_data = p\n                    break\n        else:\n            st.warning(\"No personalities available\")\n            personality_data = None\n    except Exception as e:\n        st.error(f\"Error loading personalities: {e}\")\n        personality_data = None\n\n# Main content\nif personality_data:\n    # Personality info\n    col1, col2 = st.columns([2, 1])\n    \n    with col1:\n        st.header(f\"ðŸ‘¤ {personality_data.get('name', 'Unknown')}\")\n        st.markdown(personality_data.get('description', 'No description'))\n    \n    with col2:\n        st.metric(\"Archetype\", personality_data.get('archetype', 'Unknown'))\n        st.metric(\"Version\", personality_data.get('version', 'Unknown'))\n    \n    # Chat interface\n    st.header(\"ðŸ’¬ Chat\")\n    \n    # Initialize chat history\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n    \n    # Display chat history\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n    \n    # Chat input\n    if prompt := st.chat_input(\"Type your message...\"):\n        # Add user message to chat history\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        \n        # Display user message\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n        \n        # Generate response\n        with st.chat_message(\"assistant\"):\n            with st.spinner(\"Generating response...\"):\n                try:\n                    personality = core.load_personality(selected_personality)\n                    response = personality.generate(\n                        message=prompt,\n                        provider=provider,\n                        model=model\n                    )\n                    \n                    st.markdown(response)\n                    \n                    # Add assistant response to chat history\n                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n                    \n                except Exception as e:\n                    st.error(f\"Error generating response: {e}\")\n    \n    # Clear chat button\n    if st.button(\"ðŸ—‘ï¸ Clear Chat\"):\n        st.session_state.messages = []\n        st.rerun()\n    \n    # Personality details\n    with st.expander(\"ðŸ“‹ Personality Details\"):\n        st.json(personality_data)\n\nelse:\n    st.warning(\"Please configure a personality in the sidebar\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\"Built with [Streamlit](https://streamlit.io) and [LuminoraCore](https://luminoracore.com)\")",
      "template_vars": ["project_name", "description"]
    },
    {
      "path": "requirements.txt",
      "content": "streamlit>=1.28.0\nluminoracore>=1.0.0\npython-dotenv>=1.0.0",
      "template_vars": []
    },
    {
      "path": "personalities/assistant.json",
      "content": "{\n  \"persona\": {\n    \"name\": \"AI Assistant\",\n    \"description\": \"A helpful AI assistant for web applications\",\n    \"archetype\": \"assistant\",\n    \"version\": \"1.0.0\",\n    \"author\": \"{{author}}\",\n    \"tags\": [\"web\", \"assistant\", \"helpful\"]\n  },\n  \"core_traits\": {\n    \"archetype\": \"assistant\",\n    \"temperament\": \"helpful and professional\",\n    \"communication_style\": \"clear and concise\",\n    \"values\": [\"helpfulness\", \"accuracy\", \"efficiency\"],\n    \"motivations\": [\"assisting users\", \"providing information\", \"solving problems\"]\n  },\n  \"linguistic_profile\": {\n    \"tone\": [\"professional\", \"helpful\", \"friendly\"],\n    \"vocabulary\": [\"help\", \"assist\", \"provide\", \"explain\", \"solve\", \"answer\"],\n    \"speech_patterns\": [\"I can help you with that\", \"Let me assist you\", \"Here's how we can solve this\"],\n    \"formality_level\": \"professional\",\n    \"response_length\": \"moderate\"\n  },\n  \"behavioral_rules\": [\n    \"Be helpful and professional\",\n    \"Provide clear and accurate information\",\n    \"Ask clarifying questions when needed\",\n    \"Maintain a friendly but professional tone\",\n    \"Focus on solving user problems efficiently\"\n  ],\n  \"advanced_parameters\": {\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"max_tokens\": 500,\n    \"frequency_penalty\": 0.0,\n    \"presence_penalty\": 0.0\n  }\n}",
      "template_vars": ["author"]
    },
    {
      "path": "config/luminoracore.yaml",
      "content": "# LuminoraCore Configuration\ncache_dir: ./cache\nrepository_url: https://api.luminoracore.com/v1\napi_key: null\ntimeout: 30\nmax_retries: 3\nstrict_validation: false\ndefault_provider: openai\ndefault_model: gpt-3.5-turbo\ninclude_metadata: true",
      "template_vars": []
    },
    {
      "path": "README.md",
      "content": "# {{project_name}}\n\n{{description}}\n\n## Features\n\n- Streamlit web application\n- LuminoraCore personality integration\n- Interactive chat interface\n- Multiple LLM provider support\n- Real-time personality switching\n\n## Getting Started\n\n1. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n2. Configure your API keys in `config/luminoracore.yaml`\n\n3. Run the application:\n   ```bash\n   streamlit run app.py\n   ```\n\n4. Open your browser to `http://localhost:8501`\n\n## Usage\n\n1. Select your preferred LLM provider and model in the sidebar\n2. Choose a personality to chat with\n3. Type your message and press Enter\n4. View the AI's response in the chat interface\n\n## Development\n\n```bash\n# Install development dependencies\npip install -r requirements-dev.txt\n\n# Run tests\npytest\n\n# Format code\nblack .\nisort .\n```",
      "template_vars": ["project_name", "description"]
    }
  ],
  "template_vars": {
    "project_name": {
      "type": "string",
      "description": "Name of the project",
      "default": "Streamlit LuminoraCore App"
    },
    "description": {
      "type": "string",
      "description": "Project description",
      "default": "A Streamlit web application with LuminoraCore integration"
    },
    "author": {
      "type": "string",
      "description": "Author name",
      "default": "Developer"
    }
  },
  "dependencies": [
    "streamlit>=1.28.0",
    "luminoracore>=1.0.0",
    "python-dotenv>=1.0.0"
  ],
  "dev_dependencies": [
    "pytest>=7.0.0",
    "pytest-streamlit>=0.1.0",
    "black>=23.0.0",
    "isort>=5.0.0"
  ]
}
