# üìò Gu√≠a Completa de Instalaci√≥n y Uso de LuminoraCore

Esta gu√≠a te llevar√° paso a paso desde cero hasta poder usar LuminoraCore en tu proyecto local.

## ‚ö†Ô∏è Aclaraci√≥n Importante sobre Almacenamiento

**Pregunta com√∫n:** "¬øNecesito mi propia base de datos?"

**Respuesta:** NO necesariamente. LuminoraCore ofrece M√öLTIPLES opciones:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üéØ OPCI√ìN 1: Sin Base de Datos (Por defecto)  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Storage: En memoria RAM                      ‚îÇ
‚îÇ  ‚Ä¢ Persistente: NO (se pierde al cerrar)        ‚îÇ
‚îÇ  ‚Ä¢ Instalaci√≥n: 0 pasos                         ‚îÇ
‚îÇ  ‚Ä¢ Ideal para: Pruebas, demos                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üíæ OPCI√ìN 2: Archivo JSON (Simple)  ‚ú® NUEVO  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Storage: Archivo .json o .json.gz            ‚îÇ
‚îÇ  ‚Ä¢ Persistente: S√ç (archivo en disco)           ‚îÇ
‚îÇ  ‚Ä¢ Instalaci√≥n: 0 pasos                         ‚îÇ
‚îÇ  ‚Ä¢ Ideal para: Bots personales, backups         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üì± OPCI√ìN 3: SQLite (M√≥viles)  ‚ú® NUEVO       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Storage: Archivo .db (SQLite)                ‚îÇ
‚îÇ  ‚Ä¢ Persistente: S√ç (perfecto para m√≥viles)      ‚îÇ
‚îÇ  ‚Ä¢ Instalaci√≥n: 0 pasos                         ‚îÇ
‚îÇ  ‚Ä¢ Ideal para: Apps iOS/Android, desktop        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üöÄ OPCI√ìN 4+: Con Base de Datos (Opcional)    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Storage: Redis/PostgreSQL/MongoDB            ‚îÇ
‚îÇ  ‚Ä¢ Persistente: S√ç                              ‚îÇ
‚îÇ  ‚Ä¢ Instalaci√≥n: Requiere servidor BBDD          ‚îÇ
‚îÇ  ‚Ä¢ Ideal para: Producci√≥n web, alta escala      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**üëâ Para empezar NO necesitas nada. Todo funciona en memoria.**

**üëâ Para apps m√≥viles usa SQLite (incluido, sin instalaci√≥n adicional).**

**üëâ Para persistencia simple usa JSON (sin servidor de BBDD).**

Detalles completos en: [Secci√≥n de Almacenamiento](#-almacenamiento-de-conversaciones-storage)

---

## üèóÔ∏è Arquitectura del Proyecto

LuminoraCore est√° compuesto por **3 componentes principales**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. luminoracore (Motor Base / Core Engine)         ‚îÇ
‚îÇ     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  ‚îÇ
‚îÇ     ‚Ä¢ Gesti√≥n de personalidades                     ‚îÇ
‚îÇ     ‚Ä¢ Validaci√≥n y compilaci√≥n                      ‚îÇ
‚îÇ     ‚Ä¢ PersonaBlend‚Ñ¢ Technology                      ‚îÇ
‚îÇ     ‚Ä¢ NO tiene interfaz (es una librer√≠a)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚îÇ AMBOS USAN EL MOTOR BASE
                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. CLI       ‚îÇ    ‚îÇ  3. SDK                 ‚îÇ
‚îÇ  (Terminal)   ‚îÇ    ‚îÇ  (Python Apps)          ‚îÇ
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ ‚Ä¢ Comandos    ‚îÇ    ‚îÇ ‚Ä¢ Client API            ‚îÇ
‚îÇ ‚Ä¢ Wizard      ‚îÇ    ‚îÇ ‚Ä¢ Sessions              ‚îÇ
‚îÇ ‚Ä¢ Testing     ‚îÇ    ‚îÇ ‚Ä¢ Real LLM calls        ‚îÇ
‚îÇ ‚Ä¢ Servidor    ‚îÇ    ‚îÇ ‚Ä¢ Multi-provider        ‚îÇ
‚îÇ               ‚îÇ    ‚îÇ                         ‚îÇ
‚îÇ DEPENDE DE:   ‚îÇ    ‚îÇ DEPENDE DE:             ‚îÇ
‚îÇ luminoracore  ‚îÇ    ‚îÇ luminoracore            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**‚ö†Ô∏è IMPORTANTE - Orden de Instalaci√≥n:**

```
1. PRIMERO: luminoracore (motor base)
           ‚Üì
2. DESPU√âS: luminoracore-cli (usa el motor)
           ‚Üì
3. DESPU√âS: luminoracore-sdk (usa el motor)
```

**¬øPor qu√© este orden?**
- El **CLI** importa `from luminoracore import Personality, PersonalityCompiler`
- El **SDK** importa `from luminoracore import Personality, PersonalityBlender`
- Si instalas CLI o SDK **sin** el motor base, obtendr√°s `ModuleNotFoundError`

**Dependencias t√©cnicas:**
```python
# luminoracore-cli/setup.py
install_requires=[
    'luminoracore>=0.1.0',  # ‚Üê Requiere el motor base
    'click>=8.0.0',
    ...
]

# luminoracore-sdk-python/setup.py
install_requires=[
    'luminoracore>=0.1.0',  # ‚Üê Requiere el motor base
    'aiohttp>=3.8.0',
    ...
]
```

---

## ü§î ¬øQu√© es Cada Componente?

### 1Ô∏è‚É£ **luminoracore** (Motor Base)

**Es:** Una librer√≠a Python (sin interfaz)

**Hace:**
- Carga archivos JSON de personalidades
- Valida que el JSON sea correcto
- Compila personalidades para diferentes LLMs
- Mezcla personalidades (PersonaBlend)

**NO hace:**
- ‚ùå NO tiene comandos de terminal
- ‚ùå NO hace llamadas a APIs de LLM
- ‚ùå NO tiene interfaz gr√°fica
- ‚ùå NO gestiona sesiones

**Uso t√≠pico:**
```python
# En tu c√≥digo Python
from luminoracore import Personality, PersonalityCompiler

personality = Personality("dr_luna.json")
compiler = PersonalityCompiler()
result = compiler.compile(personality, "openai")
```

**Analog√≠a:** Es como el "motor" de un coche. Funciona, pero necesitas el resto del coche para conducir.

---

### 2Ô∏è‚É£ **luminoracore-cli** (Herramienta de Terminal)

**Es:** Una herramienta de l√≠nea de comandos que **USA** el motor base

**Hace:**
- ‚úÖ Ejecutar comandos desde la terminal
- ‚úÖ Validar archivos: `luminoracore validate archivo.json`
- ‚úÖ Compilar: `luminoracore compile archivo.json`
- ‚úÖ Crear personalidades: `luminoracore create --interactive`
- ‚úÖ Listar: `luminoracore list`
- ‚úÖ Testing b√°sico

**Internamente:**
```python
# Dentro de luminoracore-cli
from luminoracore import Personality, PersonalityCompiler  # ‚Üê USA EL MOTOR

def validate_command(file_path):
    personality = Personality(file_path)  # ‚Üê Usa el motor base
    # ... resto del c√≥digo
```

**Analog√≠a:** Es como el "volante y los pedales" del coche. Te permite USAR el motor desde la terminal.

---

### 3Ô∏è‚É£ **luminoracore-sdk** (SDK para Apps)

**Es:** Un cliente completo para construir aplicaciones que **USA** el motor base

**Hace:**
- ‚úÖ Gestionar sesiones de conversaci√≥n
- ‚úÖ Hacer llamadas REALES a APIs de LLM (OpenAI, DeepSeek, etc.)
- ‚úÖ Almacenar historial de conversaciones
- ‚úÖ Gestionar memoria de sesi√≥n
- ‚úÖ Analytics y m√©tricas

**Internamente:**
```python
# Dentro de luminoracore-sdk
from luminoracore import Personality, PersonalityCompiler  # ‚Üê USA EL MOTOR

class LuminoraCoreClient:
    async def create_session(self, personality_name, provider_config):
        personality = Personality(f"{personality_name}.json")  # ‚Üê Usa el motor base
        # ... resto del c√≥digo para sesiones, LLM calls, etc.
```

**Analog√≠a:** Es como un "coche completo con GPS y sonido". Tiene el motor + todo lo necesario para una app completa.

---

## üìä Tabla Comparativa

| Caracter√≠stica | Motor Base | CLI | SDK |
|----------------|------------|-----|-----|
| **Carga personalidades** | ‚úÖ | ‚úÖ (usa motor) | ‚úÖ (usa motor) |
| **Valida JSON** | ‚úÖ | ‚úÖ (usa motor) | ‚úÖ (usa motor) |
| **Compila prompts** | ‚úÖ | ‚úÖ (usa motor) | ‚úÖ (usa motor) |
| **Comandos terminal** | ‚ùå | ‚úÖ | ‚ùå |
| **Llamadas a LLM** | ‚ùå | ‚ùå | ‚úÖ |
| **Gesti√≥n sesiones** | ‚ùå | ‚ùå | ‚úÖ |
| **Interfaz Python** | ‚úÖ | ‚ùå | ‚úÖ |
| **Wizard interactivo** | ‚ùå | ‚úÖ | ‚ùå |

---

## üéØ Respuesta a tu Pregunta

**Tu pregunta:** 
> "El CLI sirve para probar comandos de luminoracore, ¬øtiene que tener compilado o compilar luminoracore al igual que el SDK?"

**Respuesta:**

**S√ç, exactamente.** El CLI:

1. ‚úÖ **Necesita que instales primero `luminoracore`** (el motor base)
2. ‚úÖ **Importa y usa el motor base internamente**
3. ‚úÖ **No funciona si no tienes el motor base instalado**

**Lo mismo aplica para el SDK:**
- Tambi√©n necesita el motor base instalado
- Tambi√©n importa `from luminoracore import ...`

**Orden correcto de instalaci√≥n:**
```bash
# 1. PRIMERO el motor (obligatorio)
cd luminoracore
pip install -e .

# 2. DESPU√âS el CLI (opcional - solo si quieres comandos de terminal)
cd ../luminoracore-cli
pip install -e .

# 3. DESPU√âS el SDK (opcional - solo si vas a construir apps)
cd ../luminoracore-sdk-python
pip install -e .
```

**Si intentas instalar el CLI sin el motor:**
```bash
cd luminoracore-cli
pip install -e .

# ‚ùå ERROR al ejecutar comandos:
luminoracore validate archivo.json
# ModuleNotFoundError: No module named 'luminoracore'
```

---

## üìã Prerrequisitos

Antes de comenzar, aseg√∫rate de tener:

- ‚úÖ **Python 3.8 o superior** instalado
- ‚úÖ **pip** (gestor de paquetes de Python)
- ‚úÖ **git** (para clonar el repositorio)
- ‚úÖ Un editor de c√≥digo (VS Code, PyCharm, etc.)
- ‚úÖ Terminal o consola de comandos

### Verificar versiones instaladas:

```bash
python --version
# Deber√≠a mostrar: Python 3.8.x o superior

pip --version
# Deber√≠a mostrar: pip x.x.x

git --version
# Deber√≠a mostrar: git version x.x.x
```

---

## üöÄ Opci√≥n 1: Instalaci√≥n en Modo Desarrollo (Recomendado)

Esta opci√≥n te permite editar el c√≥digo fuente y ver los cambios inmediatamente.

### Paso 1: Clonar o ubicar el repositorio

Si ya tienes el proyecto descargado, navega a su carpeta:

```bash
cd "D:\Proyectos Ereace\LuminoraCoreBase"
```

Si no lo tienes, cl√≥nalo:

```bash
git clone <url-del-repositorio>
cd LuminoraCoreBase
```

### Paso 2: Crear un entorno virtual (Recomendado)

Esto a√≠sla las dependencias del proyecto:

```bash
# Crear entorno virtual
python -m venv venv

# Activar en Windows PowerShell
.\venv\Scripts\Activate.ps1

# Activar en Windows CMD
.\venv\Scripts\activate.bat

# Activar en Linux/Mac
source venv/bin/activate
```

Cuando est√© activado, ver√°s `(venv)` al inicio de tu l√≠nea de comandos.

### Paso 3: Instalar el Motor Base (luminoracore)

Este es el componente fundamental que todos los dem√°s necesitan:

```bash
# Navegar a la carpeta del motor base
cd luminoracore

# Instalar en modo desarrollo
pip install -e .

# Opcional: Instalar dependencias de desarrollo
pip install -e ".[dev]"

# Volver a la ra√≠z
cd ..
```

**¬øQu√© hace `-e`?** 
- Instala en modo "editable"
- Los cambios en el c√≥digo se reflejan inmediatamente
- No necesitas reinstalar despu√©s de cada modificaci√≥n

### Paso 4: Instalar el CLI (luminoracore-cli)

```bash
# Navegar a la carpeta del CLI
cd luminoracore-cli

# Instalar en modo desarrollo
pip install -e .

# Opcional: Dependencias extras para servidor
pip install -e ".[server]"

# Volver a la ra√≠z
cd ..
```

### Paso 5: Instalar el SDK (luminoracore-sdk-python)

```bash
# Navegar a la carpeta del SDK
cd luminoracore-sdk-python

# Instalar en modo desarrollo
pip install -e .

# Opcional: Instalar con todos los proveedores
pip install -e ".[all]"

# O solo los proveedores que necesites:
pip install -e ".[openai]"      # Solo OpenAI
pip install -e ".[anthropic]"   # Solo Anthropic
pip install -e ".[deepseek]"    # Solo DeepSeek (econ√≥mico)
pip install -e ".[mistral]"     # Solo Mistral AI
pip install -e ".[llama]"       # Solo Llama (v√≠a Replicate)
pip install -e ".[cohere]"      # Solo Cohere
pip install -e ".[google]"      # Solo Google Gemini

# Volver a la ra√≠z
cd ..
```

### Paso 6: Verificar la instalaci√≥n

```bash
# Verificar que luminoracore est√° instalado
python -c "import luminoracore; print(luminoracore.__version__)"

# Verificar que el CLI est√° disponible
luminoracore --help

# Tambi√©n puedes usar el alias corto
lc --help

# Verificar el SDK
python -c "from luminoracore import LuminoraCoreClient; print('SDK OK')"
```

---

## üéØ Opci√≥n 2: Instalaci√≥n desde PyPI (Cuando est√© publicado)

Cuando los paquetes est√©n publicados en PyPI, la instalaci√≥n ser√° m√°s simple:

```bash
# Motor base
pip install luminoracore

# CLI
pip install luminoracore-cli

# SDK con todos los proveedores
pip install luminoracore-sdk[all]
```

---

## üìù Uso Pr√°ctico - Caso 1: Usar el Motor Base (luminoracore)

### Ejemplo 1: Cargar y Validar una Personalidad

Crea un archivo `mi_ejemplo_core.py`:

```python
from luminoracore import Personality, PersonalityValidator, PersonalityCompiler, LLMProvider

# 1. Cargar una personalidad
print("1. Cargando personalidad...")
personality = Personality("personalidades/Dr. Luna Cient√≠fica Entusiasta.json")
print(f"‚úÖ Personalidad cargada: {personality.persona.name}")

# 2. Validar la personalidad
print("\n2. Validando personalidad...")
validator = PersonalityValidator()
result = validator.validate(personality)

if result.is_valid:
    print("‚úÖ Validaci√≥n exitosa")
    print(f"   - Advertencias: {len(result.warnings)}")
    print(f"   - Sugerencias: {len(result.suggestions)}")
else:
    print("‚ùå Validaci√≥n fallida:")
    for error in result.errors:
        print(f"   - {error}")

# 3. Compilar para OpenAI
print("\n3. Compilando para OpenAI...")
compiler = PersonalityCompiler()
compiled = compiler.compile(personality, LLMProvider.OPENAI)
print(f"‚úÖ Compilado exitosamente")
print(f"   - Tokens estimados: {compiled.token_estimate}")
print(f"   - Prompt (primeros 200 chars):\n{compiled.prompt[:200]}...")

# 4. Compilar para otros proveedores
print("\n4. Compilando para otros proveedores...")
for provider in [LLMProvider.ANTHROPIC, LLMProvider.DEEPSEEK, LLMProvider.LLAMA, LLMProvider.MISTRAL]:
    result = compiler.compile(personality, provider)
    print(f"‚úÖ {provider.value}: {result.token_estimate} tokens")
```

**Ejecutar:**

```bash
python mi_ejemplo_core.py
```

### Ejemplo 2: Mezclar Personalidades (PersonaBlend)

```python
from luminoracore import Personality, PersonalityBlender

# Cargar dos personalidades
print("Cargando personalidades...")
dr_luna = Personality("personalidades/Dr. Luna Cient√≠fica Entusiasta.json")
rocky = Personality("personalidades/Rocky Inspiraci√≥n.json")

# Mezclar personalidades
print("\nMezclando personalidades...")
blender = PersonalityBlender()
blended = blender.blend(
    personalities=[dr_luna, rocky],
    weights=[0.7, 0.3],
    strategy="weighted_average"
)

print(f"‚úÖ Personalidad mezclada creada: {blended.persona.name}")
print(f"   Descripci√≥n: {blended.persona.description}")
print(f"   Arqueotipo: {blended.core_traits.archetype}")
```

---

## üõ†Ô∏è Uso Pr√°ctico - Caso 2: Usar el CLI (luminoracore-cli)

El CLI te permite gestionar personalidades desde la terminal.

### Comandos B√°sicos:

```bash
# 1. Ver todas las personalidades disponibles
luminoracore list

# Con detalles
luminoracore list --detailed

# 2. Validar una personalidad
luminoracore validate "personalidades/Dr. Luna Cient√≠fica Entusiasta.json"

# Validar todas las personalidades en una carpeta
luminoracore validate personalidades/ --strict

# 3. Compilar una personalidad
luminoracore compile "personalidades/Dr. Luna Cient√≠fica Entusiasta.json" --provider openai

# Guardar en archivo
luminoracore compile "personalidades/Rocky Inspiraci√≥n.json" --provider anthropic --output rocky_prompt.txt

# 4. Crear una nueva personalidad (modo interactivo)
luminoracore create --interactive

# 5. Mezclar personalidades
luminoracore blend "personalidades/Dr. Luna Cient√≠fica Entusiasta.json:0.6" "personalidades/Rocky Inspiraci√≥n.json:0.4" --output mezcla.json

# 6. Iniciar servidor de desarrollo con interfaz web
luminoracore serve

# En puerto personalizado
luminoracore serve --port 3000

# 7. Obtener informaci√≥n de una personalidad
luminoracore info "personalidades/Victoria Sterling.json"
```

### Ejemplo Pr√°ctico: Workflow Completo

```bash
# Paso 1: Crear una nueva personalidad
luminoracore create --interactive

# Paso 2: Validar que est√© correcta
luminoracore validate mi_nueva_personalidad.json

# Paso 3: Probar compilaci√≥n para diferentes proveedores
luminoracore compile mi_nueva_personalidad.json --provider openai
luminoracore compile mi_nueva_personalidad.json --provider anthropic

# Paso 4: Iniciar servidor para pruebas visuales
luminoracore serve
# Abre http://localhost:8000 en tu navegador
```

---

## üêç Uso Pr√°ctico - Caso 3: Usar el SDK (luminoracore-sdk)

El SDK es para construir aplicaciones completas con IA.

### Ejemplo 1: Aplicaci√≥n B√°sica con OpenAI

Crea un archivo `mi_app_sdk.py`:

```python
import asyncio
import os
from luminoracore import LuminoraCoreClient
from luminoracore.types.provider import ProviderConfig
from luminoracore.types.session import StorageConfig

async def main():
    # 1. Crear configuraci√≥n del cliente
    print("1. Inicializando cliente...")
    
    # IMPORTANTE: storage_type define D√ìNDE se guardan las conversaciones
    # - "memory": En RAM (se pierde al cerrar, perfecto para pruebas)
    # - "redis": En Redis (persistente, requiere servidor Redis)
    # - "postgres": En PostgreSQL (persistente, requiere BBDD)
    # - "mongodb": En MongoDB (persistente, requiere BBDD)
    
    client = LuminoraCoreClient(
        storage_config=StorageConfig(
            storage_type="memory"  # üëà Por defecto: memoria RAM (NO persistente)
        )
    )
    
    await client.initialize()
    print("‚úÖ Cliente inicializado")
    
    # 2. Configurar proveedor LLM (OpenAI)
    print("\n2. Configurando OpenAI...")
    provider_config = ProviderConfig(
        name="openai",
        api_key=os.getenv("OPENAI_API_KEY", "tu-api-key-aqu√≠"),
        model="gpt-3.5-turbo",
        extra={
            "timeout": 30,
            "max_retries": 3
        }
    )
    print("‚úÖ Proveedor configurado")
    
    # 3. Crear una personalidad personalizada
    print("\n3. Cargando personalidad...")
    personality_data = {
        "name": "asistente_programacion",
        "description": "Un asistente experto en programaci√≥n Python",
        "system_prompt": "Eres un experto en programaci√≥n Python. Explicas conceptos de forma clara y concisa. Siempre proporcionas ejemplos de c√≥digo cuando es relevante.",
        "metadata": {
            "version": "1.0.0",
            "author": "Mi Empresa",
            "tags": ["programacion", "python", "educativo"]
        }
    }
    
    await client.load_personality("asistente_programacion", personality_data)
    print("‚úÖ Personalidad cargada")
    
    # 4. Crear una sesi√≥n
    print("\n4. Creando sesi√≥n...")
    session_id = await client.create_session(
        personality_name="asistente_programacion",
        provider_config=provider_config
    )
    print(f"‚úÖ Sesi√≥n creada: {session_id}")
    
    # 5. Enviar mensajes (ESTO HACE LLAMADAS REALES A LA API)
    print("\n5. Enviando mensaje a OpenAI...")
    
    # IMPORTANTE: Esto consumir√° tokens de tu cuenta de OpenAI
    try:
        response = await client.send_message(
            session_id=session_id,
            message="¬øPuedes explicarme qu√© son las list comprehensions en Python?"
        )
        
        print("‚úÖ Respuesta recibida:")
        print(f"   Contenido: {response.content[:200]}...")
        print(f"   Tokens usados: {response.usage}")
        print(f"   Costo estimado: ${response.cost}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error al llamar API: {e}")
        print("   (Aseg√∫rate de tener una API key v√°lida en OPENAI_API_KEY)")
    
    # 6. Ver el historial de conversaci√≥n
    print("\n6. Obteniendo historial...")
    messages = await client.get_conversation(session_id)
    print(f"‚úÖ La conversaci√≥n tiene {len(messages)} mensajes")
    
    # 7. Guardar informaci√≥n personalizada en la sesi√≥n
    print("\n7. Guardando preferencias del usuario...")
    # NOTA: Esto guarda datos ADICIONALES sobre el usuario
    # (nivel, preferencias, contexto personalizado)
    # Se guarda en el mismo storage que las conversaciones
    await client.store_memory(
        session_id=session_id,
        key="nivel_experiencia",
        value="intermedio"
    )
    print("‚úÖ Memoria guardada (se perder√° al cerrar si usas 'memory')")
    
    # 8. Limpieza
    print("\n8. Limpiando...")
    await client.cleanup()
    print("‚úÖ Limpieza completada")

# Ejecutar
if __name__ == "__main__":
    asyncio.run(main())
```

**Ejecutar:**

```bash
# Configurar tu API key
export OPENAI_API_KEY="sk-tu-api-key-aqu√≠"  # Linux/Mac
set OPENAI_API_KEY=sk-tu-api-key-aqu√≠       # Windows CMD
$env:OPENAI_API_KEY="sk-tu-api-key-aqu√≠"    # Windows PowerShell

# Ejecutar
python mi_app_sdk.py
```

### Ejemplo 2: Mezclar Personalidades en Runtime

```python
import asyncio
from luminoracore import LuminoraCoreClient
from luminoracore.types.provider import ProviderConfig

async def main():
    client = LuminoraCoreClient()
    await client.initialize()
    
    # Cargar dos personalidades diferentes
    scientist_data = {
        "name": "cient√≠fico",
        "system_prompt": "Eres un cient√≠fico riguroso que explica todo con evidencia y datos.",
        "metadata": {"version": "1.0.0"}
    }
    
    creative_data = {
        "name": "creativo",
        "system_prompt": "Eres un pensador creativo que encuentra soluciones innovadoras.",
        "metadata": {"version": "1.0.0"}
    }
    
    await client.load_personality("cient√≠fico", scientist_data)
    await client.load_personality("creativo", creative_data)
    
    # Mezclar personalidades (60% cient√≠fico, 40% creativo)
    blended = await client.blend_personalities(
        personality_names=["cient√≠fico", "creativo"],
        weights=[0.6, 0.4],
        blend_name="cient√≠fico_creativo"
    )
    
    print(f"‚úÖ Personalidad mezclada: {blended}")
    
    # Usar la personalidad mezclada
    provider_config = ProviderConfig(
        name="openai",
        api_key="tu-api-key",
        model="gpt-3.5-turbo"
    )
    
    session_id = await client.create_session(
        personality_name="cient√≠fico_creativo",
        provider_config=provider_config
    )
    
    print(f"‚úÖ Sesi√≥n con personalidad mezclada: {session_id}")
    
    await client.cleanup()

asyncio.run(main())
```

---

## üíæ Almacenamiento de Conversaciones (Storage)

### ¬øD√≥nde se guardan las conversaciones?

**Respuesta corta:** Depende de ti. LuminoraCore ofrece 4 opciones:

| Storage | Persistente | Requiere | Cu√°ndo usar |
|---------|-------------|----------|-------------|
| **memory** | ‚ùå NO | Nada | Pruebas, demos |
| **json** | ‚úÖ S√ç | Solo disco | Apps simples, backups |
| **sqlite** | ‚úÖ S√ç | Solo disco | Apps m√≥viles, desktop |
| **redis** | ‚úÖ S√ç | Servidor Redis | Producci√≥n web, alta velocidad |
| **postgres** | ‚úÖ S√ç | PostgreSQL | Producci√≥n, datos relacionales |
| **mongodb** | ‚úÖ S√ç | MongoDB | Producci√≥n, datos flexibles |

### Opci√≥n 1: Memory (Por defecto - Sin BBDD)

```python
from luminoracore import LuminoraCoreClient
from luminoracore.types.session import StorageConfig

client = LuminoraCoreClient(
    storage_config=StorageConfig(
        storage_type="memory"  # üëà En RAM
    )
)
```

**‚úÖ Ventajas:**
- No necesitas instalar nada
- Ideal para pruebas y desarrollo
- Muy r√°pido

**‚ùå Desventajas:**
- Se pierde todo al cerrar la app
- No sirve para producci√≥n
- No comparte datos entre procesos

**Cu√°ndo usar:**
- Demos y prototipos
- Testing
- Scripts de una sola ejecuci√≥n

---

### Opci√≥n 2: JSON File (Simple y Port√°til) ‚ú® NUEVO

```python
client = LuminoraCoreClient(
    storage_config=StorageConfig(
        storage_type="json",
        json_file_path="./sessions/conversations.json"  # O .json.gz comprimido
    )
)
```

**‚úÖ Ventajas:**
- Persistente (archivo en disco)
- No necesitas servidor de BBDD
- Port√°til (puedes mover el archivo)
- F√°cil de hacer backup
- Legible (puedes ver el JSON)
- Ideal para desarrollo

**‚ùå Desventajas:**
- Lento con muchas sesiones (>1000)
- No apto para m√∫ltiples procesos concurrentes
- Sin queries complejas

**Cu√°ndo usar:**
- Apps de escritorio
- Bots personales
- Scripts que se ejecutan peri√≥dicamente
- Prototipado sin complicaciones
- Backups y portabilidad

**Ejemplo con compresi√≥n:**
```python
# Guarda comprimido (ahorra espacio)
client = LuminoraCoreClient(
    storage_config=StorageConfig(
        storage_type="json",
        json_file_path="./sessions/conversations.json.gz",
        compress=True  # Comprime con gzip
    )
)
```

---

### Opci√≥n 3: SQLite (Perfecto para M√≥viles) üì± NUEVO

```python
client = LuminoraCoreClient(
    storage_config=StorageConfig(
        storage_type="sqlite",
        sqlite_path="./data/luminoracore.db"
    )
)
```

**‚úÖ Ventajas:**
- Persistente (archivo .db)
- **PERFECTO para apps m√≥viles** (iOS/Android)
- Queries SQL r√°pidas
- Ligero (solo un archivo)
- Sin servidor externo
- Transacciones ACID

**‚ùå Desventajas:**
- No apto para alta concurrencia
- Sin escalabilidad horizontal

**Cu√°ndo usar:**
- **Apps m√≥viles (iOS/Android)** ‚≠ê
- Apps de escritorio
- Prototipos que necesitan SQL
- Apps con un solo usuario

**Ejemplo para m√≥vil:**
```python
# En Android/iOS
import os
from pathlib import Path

# Ruta en el almacenamiento de la app
if platform.system() == "Android":
    db_path = Path("/data/data/com.tuapp/databases/luminoracore.db")
else:  # iOS
    db_path = Path.home() / "Documents" / "luminoracore.db"

client = LuminoraCoreClient(
    storage_config=StorageConfig(
        storage_type="sqlite",
        sqlite_path=str(db_path)
    )
)
```

---

### Opci√≥n 4: Redis (Recomendado para producci√≥n web)

```python
client = LuminoraCoreClient(
    storage_config=StorageConfig(
        storage_type="redis",
        redis_url="redis://localhost:6379",
        redis_db=0
    )
)
```

**‚úÖ Ventajas:**
- Persistente
- Muy r√°pido (en memoria)
- Perfecto para sesiones
- TTL autom√°tico

**‚ùå Desventajas:**
- Requiere servidor Redis

**Instalaci√≥n de Redis:**
```bash
# Linux/Mac (con Homebrew)
brew install redis
redis-server

# Windows (con Docker)
docker run -d -p 6379:6379 redis

# Instalar cliente Python
pip install redis
```

**Cu√°ndo usar:**
- Chatbots en producci√≥n
- Apps con m√∫ltiples usuarios
- Necesitas velocidad + persistencia

---

### Opci√≥n 5: PostgreSQL

```python
client = LuminoraCoreClient(
    storage_config=StorageConfig(
        storage_type="postgres",
        postgres_url="postgresql://user:password@localhost/luminoracore"
    )
)
```

**‚úÖ Ventajas:**
- Persistente
- Queries SQL complejas
- Backups f√°ciles

**‚ùå Desventajas:**
- M√°s lento que Redis
- Requiere BBDD PostgreSQL

**Cu√°ndo usar:**
- Ya tienes PostgreSQL
- Necesitas hacer an√°lisis SQL
- Backups y auditor√≠a importantes

---

### Opci√≥n 6: MongoDB

```python
client = LuminoraCoreClient(
    storage_config=StorageConfig(
        storage_type="mongodb",
        mongodb_url="mongodb://localhost:27017",
        mongodb_database="luminoracore"
    )
)
```

**‚úÖ Ventajas:**
- Persistente
- Esquema flexible
- Buen rendimiento

**‚ùå Desventajas:**
- Requiere servidor MongoDB

**Cu√°ndo usar:**
- Ya tienes MongoDB
- Datos no estructurados
- Escalabilidad horizontal

---

### ¬øQu√© se guarda exactamente?

**En el storage elegido se guardan:**

1. **Historial de mensajes**
   ```python
   [
     {"role": "user", "content": "Hola"},
     {"role": "assistant", "content": "¬°Hola!"}
   ]
   ```

2. **Contexto de sesi√≥n**
   ```python
   {
     "session_id": "abc123",
     "personality_name": "dr_luna",
     "created_at": "2024-10-03T10:00:00Z"
   }
   ```

3. **Memoria personalizada**
   ```python
   {
     "nivel_experiencia": "intermedio",
     "preferencias": {"idioma": "es"},
     "contexto": {...}
   }
   ```

**NO se guarda:**
- ‚ùå El archivo JSON de la personalidad (es est√°tico)
- ‚ùå Tu c√≥digo Python (es tu aplicaci√≥n)
- ‚ùå Las API keys (est√°n en variables de entorno)

---

### Ejemplo Completo: Sin BBDD vs Con Redis

#### Sin BBDD (Memory):
```python
import asyncio
from luminoracore import LuminoraCoreClient
from luminoracore.types.session import StorageConfig

async def main():
    # Opci√≥n 1: Memory (se pierde al cerrar)
    client = LuminoraCoreClient(
        storage_config=StorageConfig(storage_type="memory")
    )
    
    await client.initialize()
    session_id = await client.create_session(...)
    await client.send_message(session_id, "Hola")
    
    # ‚ö†Ô∏è Al cerrar la app, se pierde todo
    await client.cleanup()

asyncio.run(main())
```

#### Con Redis (Persistente):
```python
import asyncio
from luminoracore import LuminoraCoreClient
from luminoracore.types.session import StorageConfig

async def main():
    # Opci√≥n 2: Redis (persistente)
    client = LuminoraCoreClient(
        storage_config=StorageConfig(
            storage_type="redis",
            redis_url="redis://localhost:6379"
        )
    )
    
    await client.initialize()
    
    # Puedes retomar sesiones anteriores
    existing_session_id = "session_from_yesterday"
    await client.send_message(existing_session_id, "Hola de nuevo")
    
    # ‚úÖ Al cerrar, los datos quedan en Redis
    await client.cleanup()

asyncio.run(main())
```

---

### Decisi√≥n R√°pida

**¬øEst√°s probando?** ‚Üí Usa `memory` (sin BBDD)

**¬øApp m√≥vil (iOS/Android)?** ‚Üí Usa `sqlite` ‚≠ê **RECOMENDADO**

**¬øApp de escritorio simple?** ‚Üí Usa `json` o `sqlite`

**¬øBot personal o script?** ‚Üí Usa `json` (f√°cil y port√°til)

**¬øProducci√≥n web con muchos usuarios?** ‚Üí Usa `redis` (r√°pido + persistente)

**¬øYa tienes PostgreSQL?** ‚Üí Usa `postgres`

**¬øYa tienes MongoDB?** ‚Üí Usa `mongodb`

---

## üîë Configuraci√≥n de API Keys

### OpenAI

```bash
# Obtener tu API key en: https://platform.openai.com/api-keys

# Linux/Mac
export OPENAI_API_KEY="sk-..."

# Windows PowerShell
$env:OPENAI_API_KEY="sk-..."

# Windows CMD
set OPENAI_API_KEY=sk-...
```

### Anthropic (Claude)

```bash
# Obtener tu API key en: https://console.anthropic.com/

# Linux/Mac
export ANTHROPIC_API_KEY="sk-ant-..."

# Windows PowerShell
$env:ANTHROPIC_API_KEY="sk-ant-..."
```

### DeepSeek (Muy Econ√≥mico) üí∞ ‚ú® NUEVO

```bash
# Obtener tu API key en: https://platform.deepseek.com/
# üåü Modelo ULTRA BARATO: ~$0.14 por 1M tokens
# Popular entre desarrolladores por su precio

# Linux/Mac
export DEEPSEEK_API_KEY="sk-..."

# Windows PowerShell
$env:DEEPSEEK_API_KEY="sk-..."

# Windows CMD
set DEEPSEEK_API_KEY=sk-...
```

**¬øPor qu√© DeepSeek?**
- üí∞ **Precio:** ~20x m√°s barato que GPT-4
- ‚ö° **Velocidad:** Respuestas r√°pidas
- üéØ **Calidad:** Competitivo con GPT-3.5
- üî• **Popular:** Favorito de desarrolladores

**Uso en el SDK:**
```python
provider_config = ProviderConfig(
    name="deepseek",
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    model="deepseek-chat"  # Modelo m√°s econ√≥mico
)
```

### Cohere

```bash
# Obtener tu API key en: https://dashboard.cohere.ai/

export COHERE_API_KEY="..."
```

### Mistral AI

```bash
# Obtener tu API key en: https://console.mistral.ai/

export MISTRAL_API_KEY="..."
```

### Google Gemini

```bash
# Obtener tu API key en: https://makersuite.google.com/app/apikey

export GOOGLE_API_KEY="..."
```

### Llama (v√≠a Replicate)

```bash
# Obtener tu API key en: https://replicate.com/account/api-tokens

export REPLICATE_API_KEY="..."
```

---

## üîß Configuraci√≥n Avanzada de Providers

### üìç URLs Personalizadas de Proveedores

**IMPORTANTE:** Todas las URLs de los proveedores est√°n configurables en un archivo JSON central:

üìÅ **Ubicaci√≥n:** `luminoracore-sdk-python/luminoracore/config/provider_urls.json`

Este archivo contiene las URLs base para todos los proveedores:

```json
{
  "providers": {
    "openai": {
      "base_url": "https://api.openai.com/v1",
      "default_model": "gpt-3.5-turbo"
    },
    "anthropic": {
      "base_url": "https://api.anthropic.com/v1",
      "default_model": "claude-3-sonnet-20240229"
    },
    "deepseek": {
      "base_url": "https://api.deepseek.com/v1",
      "default_model": "deepseek-chat"
    },
    "mistral": {
      "base_url": "https://api.mistral.ai/v1",
      "default_model": "mistral-tiny"
    },
    ...
  }
}
```

### ‚ú® ¬øPor qu√© es importante esto?

1. **URLs Cambian:** Si un proveedor cambia su endpoint, solo editas el archivo JSON
2. **Nuevos Providers:** Puedes a√±adir f√°cilmente nuevos LLMs sin modificar c√≥digo
3. **Proxies/Mirrors:** Usa URLs alternativas o proxies para acceder a los LLMs
4. **Self-hosted:** Conecta a instancias locales de modelos (Ollama, LocalAI, etc.)

### üõ†Ô∏è C√≥mo Personalizar URLs

#### Opci√≥n 1: Editar el archivo de configuraci√≥n

```json
// luminoracore-sdk-python/luminoracore/config/provider_urls.json
{
  "custom_providers": {
    "mi-llm-local": {
      "name": "Mi LLM Local",
      "base_url": "http://localhost:8000/v1",
      "default_model": "local-model",
      "chat_endpoint": "/chat/completions"
    }
  }
}
```

#### Opci√≥n 2: Override en tiempo de ejecuci√≥n (Python)

```python
from luminoracore import LuminoraCoreClient
from luminoracore.types.provider import ProviderConfig

# Crear provider con URL personalizada
provider_config = ProviderConfig(
    name="openai",
    api_key="sk-...",
    base_url="https://mi-proxy.com/openai/v1",  # URL personalizada
    model="gpt-4"
)

client = LuminoraCoreClient(provider_config=provider_config)
```

### üìã Providers Disponibles

| Provider | URL Base | Modelo Default | Instalaci√≥n |
|----------|----------|----------------|-------------|
| **OpenAI** | `https://api.openai.com/v1` | `gpt-3.5-turbo` | `pip install -e ".[openai]"` |
| **Anthropic** | `https://api.anthropic.com/v1` | `claude-3-sonnet-20240229` | `pip install -e ".[anthropic]"` |
| **DeepSeek** üí∞ | `https://api.deepseek.com/v1` | `deepseek-chat` | `pip install -e ".[deepseek]"` |
| **Mistral** | `https://api.mistral.ai/v1` | `mistral-tiny` | `pip install -e ".[mistral]"` |
| **Cohere** | `https://api.cohere.ai/v1` | `command` | `pip install -e ".[cohere]"` |
| **Google** | `https://generativelanguage.googleapis.com/v1` | `gemini-pro` | `pip install -e ".[google]"` |
| **Llama** | `https://api.replicate.com/v1` | `llama-2-7b-chat` | `pip install -e ".[llama]"` |

### üéØ Casos de Uso

**1. Usar Ollama localmente:**
```python
provider_config = ProviderConfig(
    name="openai",  # Compatible con OpenAI API
    api_key="ollama",  # Dummy key
    base_url="http://localhost:11434/v1",
    model="llama2"
)
```

**2. Usar Azure OpenAI:**
```python
provider_config = ProviderConfig(
    name="openai",
    api_key=os.getenv("AZURE_OPENAI_KEY"),
    base_url="https://YOUR-RESOURCE.openai.azure.com",
    model="gpt-35-turbo"
)
```

**3. Usar proxy corporativo:**
```python
provider_config = ProviderConfig(
    name="openai",
    api_key="sk-...",
    base_url="https://proxy.company.com/openai/v1",
    model="gpt-4"
)
```

---

## üìÇ Estructura de un Proyecto T√≠pico

```
mi-proyecto/
‚îú‚îÄ‚îÄ venv/                          # Entorno virtual
‚îú‚îÄ‚îÄ personalidades/                # Tus personalidades personalizadas
‚îÇ   ‚îú‚îÄ‚îÄ asistente_ventas.json
‚îÇ   ‚îú‚îÄ‚îÄ soporte_tecnico.json
‚îÇ   ‚îî‚îÄ‚îÄ creativo_marketing.json
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ providers.yaml            # Configuraci√≥n de proveedores
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # Tu aplicaci√≥n principal
‚îÇ   ‚îú‚îÄ‚îÄ handlers.py               # L√≥gica de negocio
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Utilidades
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_personalidades.py   # Tests
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias
‚îî‚îÄ‚îÄ README.md                     # Documentaci√≥n
```

**requirements.txt:**

```txt
# Para usar solo el motor base
luminoracore>=0.1.0

# Para usar el CLI
luminoracore-cli>=1.0.0

# Para usar el SDK completo con OpenAI
luminoracore-sdk[openai]>=1.0.0

# O con todos los proveedores
luminoracore-sdk[all]>=1.0.0
```

---

## üêõ Soluci√≥n de Problemas Comunes

### Problema 1: "ModuleNotFoundError: No module named 'luminoracore'"

**Soluci√≥n:**

```bash
# Aseg√∫rate de estar en el entorno virtual correcto
.\venv\Scripts\Activate.ps1

# Reinstala el paquete
cd luminoracore
pip install -e .
cd ..
```

### Problema 2: "Command 'luminoracore' not found"

**Soluci√≥n:**

```bash
# Reinstala el CLI
cd luminoracore-cli
pip install -e .
cd ..

# Verifica que est√© en el PATH
pip show luminoracore-cli
```

### Problema 3: Error al importar el SDK

**Soluci√≥n:**

```bash
# Instala las dependencias del proveedor que est√©s usando
cd luminoracore-sdk-python
pip install -e ".[openai]"  # Para OpenAI
pip install -e ".[anthropic]"  # Para Anthropic
pip install -e ".[all]"  # Para todos
cd ..
```

### Problema 4: "Permission denied" al activar entorno virtual en Windows

**Soluci√≥n:**

```powershell
# Ejecuta esto en PowerShell como Administrador
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### Problema 5: Las personalidades no se encuentran

**Soluci√≥n:**

```python
# Usa rutas absolutas o relativas correctas
from pathlib import Path

# Obtener la ruta del proyecto
PROJECT_ROOT = Path(__file__).parent
PERSONALITIES_DIR = PROJECT_ROOT / "personalidades"

# Cargar personalidad
personality_path = PERSONALITIES_DIR / "Dr. Luna Cient√≠fica Entusiasta.json"
personality = Personality(str(personality_path))
```

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial

- **Motor Base:** `luminoracore/docs/`
- **CLI:** `luminoracore-cli/README.md`
- **SDK:** `luminoracore-sdk-python/docs/api_reference.md`

### Ejemplos Incluidos

```bash
# Ejemplos del motor base
python luminoracore/examples/basic_usage.py
python luminoracore/examples/blending_demo.py
python luminoracore/examples/multi_llm_demo.py

# Ejemplos del SDK
python luminoracore-sdk-python/examples/basic_usage.py
python luminoracore-sdk-python/examples/personality_blending.py
```

### Archivos de Referencia

- `ESTADO_ACTUAL_PROYECTO.md` - Estado del proyecto
- `CARACTERISTICAS_TECNICAS_LUMINORACORE.md` - Caracter√≠sticas t√©cnicas
- `COMO_PROBAR_WIZARD.md` - Gu√≠a para probar el wizard

---

## ‚úÖ Lista de Verificaci√≥n para Nuevos Desarrolladores

- [ ] Python 3.8+ instalado
- [ ] Entorno virtual creado y activado
- [ ] `luminoracore` instalado
- [ ] `luminoracore-cli` instalado (si lo necesitas)
- [ ] `luminoracore-sdk` instalado (si lo necesitas)
- [ ] API keys configuradas (si vas a hacer llamadas reales)
- [ ] Primer ejemplo ejecutado exitosamente
- [ ] Documentaci√≥n le√≠da

---

## üéì Pr√≥ximos Pasos

1. **Explora las personalidades incluidas** en la carpeta `personalidades/`
2. **Ejecuta los ejemplos** en `luminoracore/examples/`
3. **Crea tu primera personalidad personalizada**
4. **Integra LuminoraCore en tu aplicaci√≥n**
5. **Comparte tus personalidades con la comunidad**

---

## üí° Casos de Uso Recomendados

### Caso 1: Chatbot de Atenci√≥n al Cliente

```python
# Usa el SDK con una personalidad de soporte amigable
# Almacenamiento en Redis para persistencia
# M√©tricas y analytics incluidos
```

### Caso 2: Asistente Educativo

```python
# Usa el motor base para cambiar entre personalidades
# Profesor riguroso para ex√°menes
# Tutor amigable para aprendizaje
```

### Caso 3: Generador de Contenido

```python
# Mezcla personalidades creativas con anal√≠ticas
# Genera contenido con voz de marca consistente
```

---

## üìû Soporte

Si tienes problemas o preguntas:

1. Revisa esta gu√≠a completa
2. Consulta `ESTADO_ACTUAL_PROYECTO.md`
3. Revisa los ejemplos en `examples/`
4. Crea un issue en el repositorio

---

**¬°Listo! Ahora tienes todo lo necesario para empezar a usar LuminoraCore en tus proyectos.** üöÄ

