# Sistema de Memoria Avanzado - LuminoraCore v1.1

**Dise√±o completo del sistema de memoria epis√≥dica, clasificaci√≥n inteligente y recuperaci√≥n contextual**

---

## ‚ö†Ô∏è NOTA IMPORTANTE

Este documento describe el **sistema de memoria** de LuminoraCore v1.1.

**Modelo Conceptual (Templates/Instances/Snapshots):**
- **Templates (JSON)** definen qu√© memoria est√° habilitada (configuraci√≥n)
- **Instances (BBDD)** guardan los datos reales (facts, episodios, mensajes)
- **Snapshots (JSON)** exportan todo el estado incluyendo memoria

**Ver:** [MODELO_CONCEPTUAL_REVISADO.md](./MODELO_CONCEPTUAL_REVISADO.md) para entender c√≥mo se integra la memoria con el modelo completo.

**Datos de Memoria:**
- ‚úÖ Facts ‚Üí Se guardan en **BBDD** (NO en JSON Template)
- ‚úÖ Episodios ‚Üí Se guardan en **BBDD** (NO en JSON Template)
- ‚úÖ Embeddings ‚Üí Se guardan en **Vector Store** (NO en JSON Template)
- ‚úÖ El JSON Template solo define **configuraci√≥n** de memoria (qu√© features est√°n habilitadas)

---

## üìã Tabla de Contenidos

1. [Visi√≥n General](#visi√≥n-general)
2. [Arquitectura de Memoria](#arquitectura-de-memoria)
3. [Tipos de Memoria](#tipos-de-memoria)
4. [Memoria Epis√≥dica](#memoria-epis√≥dica)
5. [B√∫squeda Sem√°ntica (Vector Search)](#b√∫squeda-sem√°ntica)
6. [Clasificaci√≥n Inteligente](#clasificaci√≥n-inteligente)
7. [Extracci√≥n Autom√°tica de Facts](#extracci√≥n-autom√°tica-de-facts)
8. [Almacenamiento a Largo Plazo](#almacenamiento-a-largo-plazo)
9. [Recuperaci√≥n Contextual](#recuperaci√≥n-contextual)
10. [Optimizaci√≥n y Performance](#optimizaci√≥n-y-performance)

---

## Visi√≥n General

### üéØ Objetivo

**Crear un sistema de memoria que permita a las personalidades recordar conversaciones de forma humana:**
- Recordar momentos importantes (memoria epis√≥dica)
- Buscar por significado, no solo palabras exactas (vector search)
- Clasificar autom√°ticamente informaci√≥n (facts, episodios, eventos)
- Recuperar contexto relevante autom√°ticamente

### ‚ùå Problemas Actuales (v1.0)

```python
# v1.0 - Memoria b√°sica
await client.store_memory(session_id, "favorite_anime", "Naruto")  # Manual
await client.get_memory(session_id, "favorite_anime")  # Solo key-value

# Problemas:
# 1. ‚ùå Extracci√≥n manual de facts
# 2. ‚ùå No diferencia informaci√≥n importante de trivial
# 3. ‚ùå No puede buscar "recuerdas cuando hablamos de mi perro?"
# 4. ‚ùå No guarda "momentos especiales" autom√°ticamente
# 5. ‚ùå Almacenamiento sin priorizaci√≥n
```

### ‚úÖ Soluci√≥n Propuesta (v1.1)

```python
# v1.1 - Memoria inteligente
client = LuminoraCoreClient(
    memory_config=MemoryConfig(
        enable_episodic_memory=True,       # ‚Üê Episodios autom√°ticos
        enable_fact_extraction=True,        # ‚Üê Extracci√≥n autom√°tica
        enable_semantic_search=True,        # ‚Üê B√∫squeda por significado
        memory_classification="automatic"   # ‚Üê Clasificaci√≥n IA
    )
)

# Todo es autom√°tico
response = await client.send_message(
    session_id,
    "Mi perro Max muri√≥ ayer, estoy destrozado"
)

# Sistema autom√°ticamente:
# 1. ‚úÖ Extrae facts: pet_name="Max", pet_status="deceased"
# 2. ‚úÖ Detecta importancia: 9/10 (momento emocional cr√≠tico)
# 3. ‚úÖ Crea episodio: tipo="loss", tags=["sad", "pet", "grief"]
# 4. ‚úÖ Genera embedding para b√∫squeda sem√°ntica
# 5. ‚úÖ Almacena con prioridad alta
```

---

## Arquitectura de Memoria

### üèóÔ∏è Capas del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CONVERSACI√ìN (LLM)                      ‚îÇ
‚îÇ  "Recuerdas cuando hablamos de mi perro?"                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              CAPA DE RECUPERACI√ìN INTELIGENTE              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Vector Search‚îÇ  ‚îÇ Episodic     ‚îÇ  ‚îÇ Fact Retrieval  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (Sem√°ntica)  ‚îÇ  ‚îÇ Memory Query ‚îÇ  ‚îÇ (Key-Value)     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            CAPA DE CLASIFICACI√ìN Y PROCESAMIENTO           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Importance   ‚îÇ  ‚îÇ Category     ‚îÇ  ‚îÇ Sentiment       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Scoring      ‚îÇ  ‚îÇ Classification‚îÇ  ‚îÇ Analysis        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 CAPA DE EXTRACCI√ìN                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Fact         ‚îÇ  ‚îÇ Episode      ‚îÇ  ‚îÇ Entity          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Extraction   ‚îÇ  ‚îÇ Detection    ‚îÇ  ‚îÇ Recognition     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                CAPA DE ALMACENAMIENTO                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Short-term Memory (Redis)          ‚îÇ Rolling window  ‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ
‚îÇ  ‚îÇ Long-term Memory (PostgreSQL)      ‚îÇ Facts + Episodes‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ
‚îÇ  ‚îÇ Vector Store (Pinecone/pgvector)   ‚îÇ Embeddings     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Tipos de Memoria

### 1. **Memoria de Corto Plazo (Working Memory)**

**Duraci√≥n:** Sesi√≥n actual (hasta cierre)  
**Storage:** Redis / Memory  
**Contenido:** √öltimos N mensajes de la conversaci√≥n

```python
# Configuraci√≥n
working_memory_config = {
    "max_messages": 50,              # √öltimos 50 mensajes
    "max_tokens": 4000,              # O 4000 tokens (lo que ocurra primero)
    "compression": "automatic",      # Comprimir autom√°ticamente si excede
    "backend": "redis"               # Redis para velocidad
}
```

**Uso:**
- Contexto inmediato de conversaci√≥n
- Referencia a mensajes recientes
- No requiere b√∫squeda, est√° siempre disponible

---

### 2. **Memoria Sem√°ntica (Facts)**

**Duraci√≥n:** Permanente (hasta que se actualice)  
**Storage:** PostgreSQL / MongoDB  
**Contenido:** Informaci√≥n factual sobre el usuario

```python
# Estructura de un Fact
{
    "id": "fact_123",
    "user_id": "user_456",
    "session_id": "session_789",
    "category": "personal_info",        # personal_info, preferences, relationships, etc.
    "key": "favorite_anime",
    "value": "Naruto",
    "confidence": 0.95,                 # Qu√© tan seguro est√° el sistema
    "source_message_id": "msg_555",
    "first_mentioned": "2025-10-14T10:30:00Z",
    "last_updated": "2025-10-14T10:30:00Z",
    "mention_count": 1,
    "tags": ["anime", "entertainment", "preference"],
    "context": "Usuario mencion√≥ que le encanta Naruto"
}
```

**Categor√≠as de Facts:**
- `personal_info`: Nombre, edad, profesi√≥n, ubicaci√≥n
- `preferences`: Gustos, disgustos, favoritos
- `relationships`: Familia, amigos, parejas, mascotas
- `hobbies`: Actividades, intereses
- `goals`: Objetivos, aspiraciones
- `health`: Salud f√≠sica, mental
- `work`: Trabajo, estudios, carrera
- `events`: Eventos importantes ya ocurridos

**Extracci√≥n Autom√°tica:**
```python
# Input: "Soy Diego, tengo 28 a√±os y trabajo en IT. Me encanta Naruto."

# Output autom√°tico:
facts_extracted = [
    {
        "category": "personal_info",
        "key": "name",
        "value": "Diego",
        "confidence": 0.99
    },
    {
        "category": "personal_info",
        "key": "age",
        "value": 28,
        "confidence": 0.99
    },
    {
        "category": "work",
        "key": "profession",
        "value": "IT",
        "confidence": 0.95
    },
    {
        "category": "preferences",
        "key": "favorite_anime",
        "value": "Naruto",
        "confidence": 0.90
    }
]
```

---

### 3. **Memoria Epis√≥dica (Episodes)**

**Duraci√≥n:** Permanente  
**Storage:** PostgreSQL / MongoDB  
**Contenido:** Momentos importantes de la relaci√≥n

```python
# Estructura de un Episodio
{
    "id": "episode_123",
    "user_id": "user_456",
    "session_id": "session_789",
    "type": "emotional_moment",      # emotional_moment, milestone, confession, conflict, achievement
    "title": "P√©rdida de mascota Max",
    "summary": "Usuario comparti√≥ la triste noticia de que su perro Max falleci√≥ ayer. Est√° muy afectado emocionalmente.",
    "importance": 9.5,               # 0-10 (10 = m√°s importante)
    "sentiment": "very_sad",         # very_happy, happy, neutral, sad, very_sad, angry
    "tags": ["sad", "loss", "pet", "grief", "max"],
    "participants": ["user_456", "personality_alicia"],
    "context_messages": [            # Mensajes que forman el episodio
        "msg_100",
        "msg_101",
        "msg_102"
    ],
    "timestamp": "2025-10-14T10:30:00Z",
    "temporal_decay": 1.0,          # Empieza en 1.0, decae con tiempo
    "related_facts": ["fact_pet_max", "fact_pet_status"],
    "related_episodes": [],
    "embedding": [0.234, -0.567, ...] # Para b√∫squeda sem√°ntica
}
```

**Tipos de Episodios:**

| Tipo | Descripci√≥n | Importancia Base | Ejemplos |
|------|-------------|------------------|----------|
| `emotional_moment` | Momentos de alta carga emocional | 7-10 | P√©rdidas, rupturas, confesiones |
| `milestone` | Hitos en la relaci√≥n | 6-9 | Primera conversaci√≥n, aniversarios |
| `confession` | Usuario comparte algo personal | 6-8 | Secretos, miedos, sue√±os |
| `conflict` | Desacuerdos o tensiones | 5-7 | Discusiones, malentendidos |
| `achievement` | Logros del usuario | 5-8 | Promoci√≥n, graduaci√≥n, √©xito |
| `bonding` | Momentos de conexi√≥n especial | 6-8 | Risas compartidas, apoyo mutuo |
| `routine` | Conversaciones cotidianas | 1-3 | Saludos, clima, small talk |

**Detecci√≥n Autom√°tica:**
```python
# Sistema analiza cada N mensajes (ej. cada 5 mensajes)
def detect_episode(messages: List[Message]) -> Optional[Episode]:
    # 1. An√°lisis de sentimiento
    sentiment_score = analyze_sentiment(messages)
    
    # 2. An√°lisis de importancia
    importance = score_importance(messages, sentiment_score)
    
    # 3. Si importancia > threshold, crear episodio
    if importance >= 7.0:
        episode_type = classify_episode_type(messages, sentiment_score)
        summary = generate_summary(messages)
        tags = extract_tags(messages)
        
        return Episode(
            type=episode_type,
            summary=summary,
            importance=importance,
            sentiment=sentiment_score,
            tags=tags,
            context_messages=messages
        )
    
    return None
```

---

### 4. **Memoria de Vector (Semantic Memory)**

**Duraci√≥n:** Permanente  
**Storage:** Pinecone / Weaviate / PostgreSQL pgvector  
**Contenido:** Embeddings de mensajes para b√∫squeda sem√°ntica

```python
# Cada mensaje se convierte en vector
{
    "id": "vec_msg_123",
    "message_id": "msg_123",
    "user_id": "user_456",
    "session_id": "session_789",
    "content": "Mi perro Max muri√≥ ayer",
    "embedding": [0.234, -0.567, 0.123, ...],  # 1536 dimensiones (OpenAI)
    "metadata": {
        "timestamp": "2025-10-14T10:30:00Z",
        "speaker": "user",
        "sentiment": "very_sad",
        "importance": 9.5,
        "tags": ["pet", "loss", "sad"]
    }
}
```

**B√∫squeda Sem√°ntica:**
```python
# Query: "Recuerdas cuando hablamos de mi perro?"
query_embedding = create_embedding("Recuerdas cuando hablamos de mi perro?")

# B√∫squeda por similitud coseno
results = vector_store.query(
    vector=query_embedding,
    top_k=5,
    filter={"user_id": "user_456"}
)

# Resultados similares sem√°nticamente:
# 1. "Mi perro Max muri√≥ ayer" (similitud: 0.92)
# 2. "Max era mi mejor amigo" (similitud: 0.87)
# 3. "Extra√±o mucho a mi perrito" (similitud: 0.84)
```

---

## Memoria Epis√≥dica

### üéØ Concepto

**Inspirado en memoria humana:**  
Los humanos no recordamos cada conversaci√≥n palabra por palabra, pero s√≠ recordamos **momentos especiales**.

**Ejemplos:**
- "Recuerdo cuando me contaste que tu perro muri√≥"
- "Aquella vez que te sentiste tan feliz por tu promoci√≥n"
- "Cuando me confesaste tus miedos sobre tu relaci√≥n"

### üèóÔ∏è Implementaci√≥n

```python
# luminoracore/core/memory/episodic.py

from dataclasses import dataclass
from typing import List, Optional
from datetime import datetime
import numpy as np

@dataclass
class Episode:
    """Representa un episodio memorable en la conversaci√≥n"""
    
    id: str
    user_id: str
    session_id: str
    type: str  # emotional_moment, milestone, confession, etc.
    title: str
    summary: str
    importance: float  # 0-10
    sentiment: str
    tags: List[str]
    context_messages: List[str]  # IDs de mensajes
    timestamp: datetime
    temporal_decay: float = 1.0
    related_facts: List[str] = None
    related_episodes: List[str] = None
    embedding: np.ndarray = None
    
    def get_current_importance(self) -> float:
        """Importancia actual considerando decay temporal"""
        return self.importance * self.temporal_decay
    
    def update_decay(self, days_passed: int):
        """Actualiza decay temporal (memories fade over time)"""
        # Decay logar√≠tmico: eventos recientes decaen lento
        decay_rate = 0.1
        self.temporal_decay = 1.0 / (1.0 + decay_rate * np.log(days_passed + 1))


class EpisodicMemoryManager:
    """Gestiona memoria epis√≥dica"""
    
    def __init__(
        self,
        storage_backend,
        llm_provider,
        importance_threshold: float = 7.0,
        max_episodes_per_session: int = 50
    ):
        self.storage = storage_backend
        self.llm = llm_provider
        self.importance_threshold = importance_threshold
        self.max_episodes = max_episodes_per_session
    
    async def detect_episode(
        self,
        messages: List[Message],
        context: dict
    ) -> Optional[Episode]:
        """
        Detecta si los mensajes recientes forman un episodio memorable
        
        Args:
            messages: √öltimos 3-10 mensajes
            context: Contexto adicional (afinidad, mood, etc.)
        
        Returns:
            Episode si se detecta, None si no
        """
        # 1. An√°lisis de sentimiento
        sentiment_analysis = await self._analyze_sentiment(messages)
        
        # 2. Scoring de importancia
        importance = await self._score_importance(
            messages,
            sentiment_analysis,
            context
        )
        
        # 3. Si no alcanza threshold, no crear episodio
        if importance < self.importance_threshold:
            return None
        
        # 4. Clasificar tipo de episodio
        episode_type = await self._classify_episode_type(
            messages,
            sentiment_analysis
        )
        
        # 5. Generar resumen
        summary = await self._generate_summary(messages)
        
        # 6. Extraer tags
        tags = await self._extract_tags(messages, sentiment_analysis)
        
        # 7. Crear embedding
        embedding = await self._create_embedding(summary)
        
        # 8. Crear episodio
        episode = Episode(
            id=generate_id("episode"),
            user_id=context["user_id"],
            session_id=context["session_id"],
            type=episode_type,
            title=self._generate_title(episode_type, summary),
            summary=summary,
            importance=importance,
            sentiment=sentiment_analysis["primary_emotion"],
            tags=tags,
            context_messages=[msg.id for msg in messages],
            timestamp=datetime.utcnow(),
            embedding=embedding
        )
        
        return episode
    
    async def _score_importance(
        self,
        messages: List[Message],
        sentiment: dict,
        context: dict
    ) -> float:
        """
        Calcula importancia del episodio (0-10)
        
        Factores:
        - Intensidad emocional (40%)
        - Revelaci√≥n personal (30%)
        - Impacto en relaci√≥n (20%)
        - Singularidad del tema (10%)
        """
        # Usar LLM para scoring
        prompt = f"""
        Analiza la importancia de esta conversaci√≥n en una escala de 0-10.
        
        Conversaci√≥n:
        {format_messages(messages)}
        
        Factores a considerar:
        - Intensidad emocional: {sentiment['intensity']}
        - Emoci√≥n principal: {sentiment['primary_emotion']}
        - Contexto: Nivel de afinidad {context.get('affinity', 0)}/100
        
        Responde con JSON:
        {{
            "importance_score": 0-10,
            "reasoning": "explicaci√≥n breve",
            "key_factors": ["factor1", "factor2"]
        }}
        """
        
        result = await self.llm.complete(
            prompt,
            response_format="json_object"
        )
        
        return result["importance_score"]
    
    async def retrieve_relevant_episodes(
        self,
        query: str,
        user_id: str,
        top_k: int = 5,
        min_importance: float = 5.0
    ) -> List[Episode]:
        """
        Recupera episodios relevantes para una query
        
        Args:
            query: Consulta del usuario (ej. "cuando hablamos de mi perro")
            user_id: ID del usuario
            top_k: Cu√°ntos episodios retornar
            min_importance: Importancia m√≠nima a considerar
        
        Returns:
            Lista de episodios relevantes ordenados por relevancia
        """
        # 1. Crear embedding de la query
        query_embedding = await self._create_embedding(query)
        
        # 2. Buscar episodios similares
        similar_episodes = await self.storage.vector_search(
            collection="episodes",
            vector=query_embedding,
            filter={
                "user_id": user_id,
                "current_importance": {"$gte": min_importance}
            },
            top_k=top_k * 2  # Obtener m√°s para re-ranking
        )
        
        # 3. Re-ranking considerando temporal decay
        current_time = datetime.utcnow()
        for episode in similar_episodes:
            days_passed = (current_time - episode.timestamp).days
            episode.update_decay(days_passed)
        
        # 4. Ordenar por importancia actual
        sorted_episodes = sorted(
            similar_episodes,
            key=lambda e: e.get_current_importance(),
            reverse=True
        )
        
        return sorted_episodes[:top_k]
```

### üìä Ejemplo de Uso

```python
# Configuraci√≥n
client = LuminoraCoreClient(
    memory_config=MemoryConfig(
        enable_episodic_memory=True,
        episode_importance_threshold=7.0,
        episode_detection_frequency=5  # Cada 5 mensajes
    )
)

# Conversaci√≥n
session_id = await client.create_session(...)

# Mensaje 1
await client.send_message(session_id, "Hola, ¬øc√≥mo est√°s?")
# ‚Üí No se detecta episodio (rutina)

# Mensaje 2-6 (conversaci√≥n emocional)
await client.send_message(session_id, "Tengo que contarte algo triste")
await client.send_message(session_id, "Mi perro Max muri√≥ ayer")
await client.send_message(session_id, "Estoy destrozado, era mi mejor amigo")
await client.send_message(session_id, "No s√© c√≥mo superarlo")
# ‚Üí Sistema detecta episodio de importancia 9.5/10

# Weeks later...
await client.send_message(session_id, "Recuerdas cuando te habl√© de Max?")

# Sistema autom√°ticamente:
# 1. Busca episodios sem√°nticamente similares
# 2. Encuentra episodio "P√©rdida de mascota Max"
# 3. Lo incluye en contexto del LLM
# 4. LLM puede referirse espec√≠ficamente al episodio

# Respuesta: "Claro que s√≠, recuerdo cuando me contaste de Max hace 2 semanas.
#             S√© que era muy importante para ti. ¬øC√≥mo te sientes ahora?"
```

---

## B√∫squeda Sem√°ntica

### üîç Vector Search

**Problema:** B√∫squeda exacta no funciona para memoria conversacional

```python
# ‚ùå B√∫squeda exacta
user: "cuando hablamos de mi perro"
system.search("perro")  # Solo encuentra mensajes con palabra "perro"

user: "aquella vez que te cont√© de mi mascota"
system.search("mascota")  # No encuentra nada si dijiste "perro" antes
```

**Soluci√≥n:** B√∫squeda sem√°ntica por embeddings

```python
# ‚úÖ B√∫squeda sem√°ntica
user: "cuando hablamos de mi perro"
embedding = create_embedding("cuando hablamos de mi perro")
results = vector_search(embedding)
# ‚Üí Encuentra: "mi perro Max", "mi mascota", "mi perrito", etc.

user: "aquella vez que te cont√© de mi mascota"
embedding = create_embedding("aquella vez que te cont√© de mi mascota")
results = vector_search(embedding)
# ‚Üí Encuentra conversaciones sobre perros, gatos, pets en general
```

### üèóÔ∏è Implementaci√≥n

```python
# luminoracore/core/memory/semantic.py

from typing import List, Optional
import numpy as np

class SemanticMemoryManager:
    """Gestiona b√∫squeda sem√°ntica en memoria"""
    
    def __init__(
        self,
        embedding_provider: str = "openai",  # openai, cohere, sentence-transformers
        vector_store: str = "pgvector",      # pgvector, pinecone, weaviate
        embedding_model: str = "text-embedding-3-small",
        similarity_threshold: float = 0.75
    ):
        self.embedding_provider = self._init_embedding_provider(
            embedding_provider,
            embedding_model
        )
        self.vector_store = self._init_vector_store(vector_store)
        self.similarity_threshold = similarity_threshold
    
    async def index_message(
        self,
        message: Message,
        metadata: dict
    ) -> str:
        """
        Indexa un mensaje para b√∫squeda sem√°ntica
        
        Args:
            message: Mensaje a indexar
            metadata: Metadata adicional (timestamp, speaker, sentiment, etc.)
        
        Returns:
            ID del vector indexado
        """
        # 1. Crear embedding
        embedding = await self.embedding_provider.create_embedding(
            message.content
        )
        
        # 2. Preparar metadata
        full_metadata = {
            "message_id": message.id,
            "user_id": message.user_id,
            "session_id": message.session_id,
            "content": message.content,
            "timestamp": message.timestamp.isoformat(),
            "speaker": message.speaker,
            **metadata
        }
        
        # 3. Indexar en vector store
        vector_id = await self.vector_store.upsert(
            id=f"vec_{message.id}",
            vector=embedding,
            metadata=full_metadata
        )
        
        return vector_id
    
    async def search(
        self,
        query: str,
        user_id: str,
        top_k: int = 10,
        filter: Optional[dict] = None
    ) -> List[dict]:
        """
        Busca mensajes sem√°nticamente similares
        
        Args:
            query: Consulta en lenguaje natural
            user_id: ID del usuario
            top_k: N√∫mero de resultados
            filter: Filtros adicionales (tiempo, tags, etc.)
        
        Returns:
            Lista de mensajes ordenados por relevancia
        """
        # 1. Crear embedding de la query
        query_embedding = await self.embedding_provider.create_embedding(query)
        
        # 2. Preparar filtros
        search_filter = {"user_id": user_id}
        if filter:
            search_filter.update(filter)
        
        # 3. B√∫squeda vectorial
        results = await self.vector_store.query(
            vector=query_embedding,
            top_k=top_k,
            filter=search_filter,
            include_metadata=True
        )
        
        # 4. Filtrar por threshold de similitud
        filtered_results = [
            r for r in results
            if r["score"] >= self.similarity_threshold
        ]
        
        return filtered_results
    
    async def search_with_temporal_boost(
        self,
        query: str,
        user_id: str,
        top_k: int = 10,
        recency_weight: float = 0.3
    ) -> List[dict]:
        """
        B√∫squeda sem√°ntica con boost por recencia
        
        Mensajes recientes tienen score boost
        """
        results = await self.search(query, user_id, top_k * 2)
        
        current_time = datetime.utcnow()
        
        for result in results:
            # Calcular recencia (0-1, m√°s reciente = mayor)
            timestamp = datetime.fromisoformat(result["metadata"]["timestamp"])
            days_ago = (current_time - timestamp).days
            recency_score = 1.0 / (1.0 + 0.1 * days_ago)
            
            # Combinar similarity + recency
            original_score = result["score"]
            boosted_score = (
                (1 - recency_weight) * original_score +
                recency_weight * recency_score
            )
            result["score"] = boosted_score
        
        # Re-ordenar por score boosted
        sorted_results = sorted(
            results,
            key=lambda r: r["score"],
            reverse=True
        )
        
        return sorted_results[:top_k]
```

### üéØ Ejemplo de Uso

```python
# Indexar mensajes autom√°ticamente
@client.on_message
async def on_new_message(message: Message):
    # Autom√°tico en v1.1
    await semantic_memory.index_message(
        message,
        metadata={
            "sentiment": analyze_sentiment(message),
            "tags": extract_tags(message),
            "importance": score_importance(message)
        }
    )

# B√∫squeda sem√°ntica
results = await client.search_memories(
    session_id=session_id,
    query="cuando hablamos de mi perro",
    top_k=5
)

# Resultados:
# [
#   {
#     "content": "Mi perro Max muri√≥ ayer",
#     "score": 0.92,
#     "timestamp": "2025-10-01T10:30:00Z"
#   },
#   {
#     "content": "Max era un golden retriever de 10 a√±os",
#     "score": 0.88,
#     "timestamp": "2025-10-01T10:35:00Z"
#   },
#   ...
# ]
```

---

## Clasificaci√≥n Inteligente

### üìä Sistema de Clasificaci√≥n Multi-dimensional

**Cada memoria se clasifica en:**

1. **Categor√≠a** (qu√© tipo de informaci√≥n)
2. **Importancia** (qu√© tan relevante)
3. **Sentimiento** (qu√© emoci√≥n)
4. **Temporalidad** (cu√°ndo ocurri√≥ / validez temporal)
5. **Privacidad** (qu√© tan sensible)

```python
# luminoracore/core/memory/classifier.py

from enum import Enum
from dataclasses import dataclass

class MemoryCategory(Enum):
    PERSONAL_INFO = "personal_info"      # Nombre, edad, profesi√≥n
    PREFERENCES = "preferences"          # Gustos, disgustos
    RELATIONSHIPS = "relationships"      # Familia, amigos, pareja
    HOBBIES = "hobbies"                 # Actividades, intereses
    GOALS = "goals"                     # Objetivos, aspiraciones
    HEALTH = "health"                   # Salud f√≠sica, mental
    WORK = "work"                       # Trabajo, estudios
    EVENTS = "events"                   # Eventos pasados
    ROUTINE = "routine"                 # H√°bitos, rutinas
    OTHER = "other"

class ImportanceLevel(Enum):
    CRITICAL = 9-10      # Eventos life-changing
    HIGH = 7-8           # Muy importante
    MEDIUM = 5-6         # Moderadamente importante
    LOW = 3-4            # Poco importante
    TRIVIAL = 0-2        # Irrelevante

class SentimentLevel(Enum):
    VERY_POSITIVE = "very_positive"
    POSITIVE = "positive"
    NEUTRAL = "neutral"
    NEGATIVE = "negative"
    VERY_NEGATIVE = "very_negative"

class PrivacyLevel(Enum):
    PUBLIC = "public"            # Info p√∫blica
    PRIVATE = "private"          # Info personal no sensible
    SENSITIVE = "sensitive"      # Info sensible (salud, finanzas)
    CONFIDENTIAL = "confidential"  # Info muy privada (secretos)

@dataclass
class MemoryClassification:
    category: MemoryCategory
    importance: float  # 0-10
    sentiment: SentimentLevel
    privacy: PrivacyLevel
    temporal_relevance: float  # 0-1 (1 = siempre relevante, 0 = ya no relevante)
    confidence: float  # 0-1 (confianza en la clasificaci√≥n)
    tags: List[str]


class MemoryClassifier:
    """Clasifica memories autom√°ticamente usando LLM"""
    
    def __init__(self, llm_provider):
        self.llm = llm_provider
    
    async def classify(
        self,
        content: str,
        context: Optional[dict] = None
    ) -> MemoryClassification:
        """
        Clasifica un contenido de memoria
        
        Args:
            content: Texto a clasificar
            context: Contexto adicional
        
        Returns:
            MemoryClassification con todos los atributos
        """
        prompt = f"""
        Clasifica la siguiente informaci√≥n de memoria del usuario.
        
        Contenido: "{content}"
        {f"Contexto: {context}" if context else ""}
        
        Responde con JSON:
        {{
            "category": "personal_info | preferences | relationships | hobbies | goals | health | work | events | routine | other",
            "importance": 0-10,
            "importance_reasoning": "explicaci√≥n breve",
            "sentiment": "very_positive | positive | neutral | negative | very_negative",
            "privacy": "public | private | sensitive | confidential",
            "temporal_relevance": 0-1,
            "tags": ["tag1", "tag2", "tag3"],
            "confidence": 0-1
        }}
        
        Criterios de importancia:
        - 9-10: Eventos life-changing (muerte, nacimiento, matrimonio, divorcio)
        - 7-8: Muy importante (cambio de trabajo, mudanza, enfermedad seria)
        - 5-6: Moderadamente importante (nueva relaci√≥n, hobby nuevo)
        - 3-4: Poco importante (preferencia de comida, opini√≥n)
        - 0-2: Trivial (clima, saludo)
        
        Criterios de privacidad:
        - Confidential: Secretos, traumas, info financiera sensible
        - Sensitive: Salud mental, problemas personales
        - Private: Info personal normal (edad, trabajo, gustos)
        - Public: Info que el usuario compartir√≠a p√∫blicamente
        
        Temporal relevance:
        - 1.0: Siempre relevante (nombre, profesi√≥n, valores)
        - 0.7: Relevante por a√±os (trabajo actual, relaci√≥n actual)
        - 0.5: Relevante por meses (proyecto actual, meta a corto plazo)
        - 0.3: Relevante por semanas (mood temporal, evento pr√≥ximo)
        - 0.0: Ya no relevante (evento pasado √∫nico, mood pasajero)
        """
        
        result = await self.llm.complete(
            prompt,
            response_format="json_object",
            temperature=0.1  # Baja temperatura para consistencia
        )
        
        return MemoryClassification(
            category=MemoryCategory(result["category"]),
            importance=result["importance"],
            sentiment=SentimentLevel(result["sentiment"]),
            privacy=PrivacyLevel(result["privacy"]),
            temporal_relevance=result["temporal_relevance"],
            confidence=result["confidence"],
            tags=result["tags"]
        )
```

### üéØ Uso de Clasificaci√≥n

```python
# Autom√°tico al guardar memoria
async def store_memory_with_classification(
    content: str,
    user_id: str,
    session_id: str
):
    # 1. Clasificar
    classification = await classifier.classify(content)
    
    # 2. Decidir storage strategy seg√∫n clasificaci√≥n
    if classification.importance >= 7.0:
        # Alta importancia ‚Üí crear episodio
        episode = await episodic_memory.create_episode(content, classification)
    
    if classification.category == MemoryCategory.PERSONAL_INFO:
        # Info personal ‚Üí extraer facts
        facts = await fact_extractor.extract(content)
        await storage.save_facts(facts)
    
    # 3. Indexar para b√∫squeda sem√°ntica
    await semantic_memory.index(
        content,
        metadata={
            "category": classification.category.value,
            "importance": classification.importance,
            "sentiment": classification.sentiment.value,
            "privacy": classification.privacy.value,
            "tags": classification.tags
        }
    )
    
    # 4. Guardar clasificaci√≥n
    await storage.save_classification(content, classification)
```

---

## Extracci√≥n Autom√°tica de Facts

### ü§ñ NLP-Based Fact Extraction

```python
# luminoracore/core/memory/fact_extractor.py

class FactExtractor:
    """Extrae facts autom√°ticamente de conversaciones"""
    
    def __init__(self, llm_provider, confidence_threshold: float = 0.7):
        self.llm = llm_provider
        self.confidence_threshold = confidence_threshold
    
    async def extract_from_message(
        self,
        message: str,
        context: Optional[List[Message]] = None
    ) -> List[Fact]:
        """
        Extrae facts de un mensaje
        
        Args:
            message: Mensaje del usuario
            context: Mensajes anteriores para contexto
        
        Returns:
            Lista de facts extra√≠dos
        """
        context_str = ""
        if context:
            context_str = "\n".join([f"{m.speaker}: {m.content}" for m in context[-5:]])
        
        prompt = f"""
        Extrae informaci√≥n factual sobre el usuario del siguiente mensaje.
        
        {f"Contexto previo:\n{context_str}\n" if context else ""}
        
        Mensaje del usuario: "{message}"
        
        Responde con JSON:
        {{
            "facts": [
                {{
                    "category": "personal_info | preferences | relationships | hobbies | goals | health | work",
                    "key": "nombre_descriptivo_del_fact",
                    "value": "valor_extra√≠do",
                    "confidence": 0-1,
                    "reasoning": "por qu√© extra√≠ste este fact"
                }}
            ]
        }}
        
        Reglas:
        - Solo extrae facts EXPL√çCITOS, no infieras
        - Confidence alto (>0.9) solo si es statement directo
        - Key debe ser descriptivo (ej. "favorite_anime", "pet_name", "age")
        - Si no hay facts, retorna array vac√≠o
        
        Ejemplos:
        
        Input: "Soy Diego, tengo 28 a√±os y trabajo en IT"
        Output:
        {{
            "facts": [
                {{"category": "personal_info", "key": "name", "value": "Diego", "confidence": 0.99, "reasoning": "Usuario declar√≥ su nombre directamente"}},
                {{"category": "personal_info", "key": "age", "value": 28, "confidence": 0.99, "reasoning": "Usuario declar√≥ su edad directamente"}},
                {{"category": "work", "key": "profession", "value": "IT", "confidence": 0.95, "reasoning": "Usuario declar√≥ su profesi√≥n"}}
            ]
        }}
        
        Input: "Me encanta Naruto"
        Output:
        {{
            "facts": [
                {{"category": "preferences", "key": "favorite_anime", "value": "Naruto", "confidence": 0.90, "reasoning": "Usuario expres√≥ fuerte preferencia"}}
            ]
        }}
        
        Input: "Hace calor hoy"
        Output:
        {{
            "facts": []
        }}
        """
        
        result = await self.llm.complete(
            prompt,
            response_format="json_object",
            temperature=0.1
        )
        
        # Filtrar por confidence threshold
        facts = [
            Fact(**f)
            for f in result["facts"]
            if f["confidence"] >= self.confidence_threshold
        ]
        
        return facts
    
    async def extract_from_conversation(
        self,
        messages: List[Message],
        batch_size: int = 10
    ) -> List[Fact]:
        """
        Extrae facts de una conversaci√≥n completa
        
        Procesa en batches para no exceder context window
        """
        all_facts = []
        
        for i in range(0, len(messages), batch_size):
            batch = messages[i:i+batch_size]
            batch_text = "\n".join([f"{m.speaker}: {m.content}" for m in batch])
            
            facts = await self.extract_from_message(
                batch_text,
                context=messages[max(0, i-5):i] if i > 0 else None
            )
            
            all_facts.extend(facts)
        
        # Deduplicar facts
        deduplicated = self._deduplicate_facts(all_facts)
        
        return deduplicated
    
    def _deduplicate_facts(self, facts: List[Fact]) -> List[Fact]:
        """
        Elimina facts duplicados, manteniendo el de mayor confidence
        """
        facts_by_key = {}
        
        for fact in facts:
            key = f"{fact.category}:{fact.key}"
            
            if key not in facts_by_key or fact.confidence > facts_by_key[key].confidence:
                facts_by_key[key] = fact
        
        return list(facts_by_key.values())
```

### üéØ Ejemplo de Uso

```python
# Habilitado por defecto en v1.1
client = LuminoraCoreClient(
    memory_config=MemoryConfig(
        enable_fact_extraction=True,
        fact_confidence_threshold=0.7
    )
)

# El usuario habla
response = await client.send_message(
    session_id,
    "Hola! Soy Diego, tengo 28 a√±os. Trabajo en IT y me encanta Naruto. Tengo un perro llamado Max."
)

# Sistema autom√°ticamente extrae:
# Facts:
# 1. {category: "personal_info", key: "name", value: "Diego", confidence: 0.99}
# 2. {category: "personal_info", key: "age", value: 28, confidence: 0.99}
# 3. {category: "work", key: "profession", value: "IT", confidence: 0.95}
# 4. {category: "preferences", key: "favorite_anime", value: "Naruto", confidence: 0.90}
# 5. {category: "relationships", key: "pet_name", value: "Max", confidence: 0.95}
# 6. {category: "relationships", key: "pet_type", value: "dog", confidence: 0.95}

# Guardar facts
await storage.save_facts(facts, user_id, session_id)

# Luego, en conversaci√≥n futura:
response = await client.send_message(
    session_id,
    "¬øC√≥mo est√° tu perro?"
)

# Sistema autom√°ticamente:
# 1. Recupera fact: pet_name = "Max"
# 2. Inyecta en contexto: "El usuario tiene un perro llamado Max"
# 3. LLM responde: "¬øC√≥mo est√° Max? üê∂"
```

---

**(Contin√∫a en siguiente mensaje debido a l√≠mite de longitud...)**


